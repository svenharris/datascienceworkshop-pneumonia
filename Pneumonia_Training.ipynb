{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pneumonia_Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* v001_Initial commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loren\\Anaconda3\\envs\\rsna\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import itertools\n",
    "from scipy import misc\n",
    "\n",
    "# keras\n",
    "# from keras.applications.inception_v3 import InceptionV3\n",
    "# from keras.preprocessing import image\n",
    "# from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clf', 'negative', 'positive']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA_DIR = os.path.join(os.path.expandvars('%HOMEPATH%'), 'Downloads/Data')\n",
    "DATA_DIR = os.path.join('C:/dev/datascienceworkshop-pneumonia/data')\n",
    "\n",
    "TRAIN_DCM = os.path.join(DATA_DIR, \"stage_1_train_images\")\n",
    "TEST_DCM = os.path.join(DATA_DIR, \"stage_1_test_images\")\n",
    "TRAIN_IMAGES = os.path.join(DATA_DIR, \"train_images\")\n",
    "TEST_IMAGES = os.path.join(DATA_DIR, \"test_images\")\n",
    "TRAIN_IMAGES_POSITIVE = os.path.join(TRAIN_IMAGES, 'positive')\n",
    "TRAIN_IMAGES_NEGATIVE = os.path.join(TRAIN_IMAGES, 'negative')\n",
    "\n",
    "TRAIN_FEATURES_DIR = 'D:/Google Drive/Colab Notebooks/features'# os.path.join(TRAIN_IMAGES, 'features')\n",
    "CLF_DIR = 'D:/Google Drive/Colab Notebooks/features/clf'\n",
    "\n",
    "os.listdir(TRAIN_FEATURES_DIR)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features using NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import                VGG16\n",
    "from keras.applications.vgg19 import                VGG19\n",
    "from keras.applications.resnet50 import             ResNet50\n",
    "from keras.applications.xception import             Xception\n",
    "from keras.applications.inception_resnet_v2 import  InceptionResNetV2\n",
    "from keras.applications.inception_v3 import         InceptionV3\n",
    "from keras.applications.mobilenet import            MobileNet\n",
    "from keras.applications.mobilenetv2 import          MobileNetV2\n",
    "from keras.applications.densenet import             DenseNet121\n",
    "from keras.applications.densenet import             DenseNet169\n",
    "from keras.applications.densenet import             DenseNet201\n",
    "from keras.applications.nasnet import               NASNetLarge\n",
    "from keras.applications.nasnet import               NASNetMobile\n",
    "\n",
    "class Model(object):\n",
    "\n",
    "    def __init__(self, name, architecture, input_size, include_top, pooling):\n",
    "        self.Name = name\n",
    "        self.Architecture = architecture\n",
    "        self.Input_size = input_size\n",
    "        self.Include_top = include_top\n",
    "        self.Pooling = pooling\n",
    "        self.Model = None\n",
    "\n",
    "    def CreateModel(self):\n",
    "        self.Model = self.Architecture(weights='imagenet', include_top=self.Include_top, pooling=self.Pooling)\n",
    "\n",
    "\n",
    "architectures = {\n",
    "    \"VGG16\":VGG16,\n",
    "    \"VGG19\":VGG19,\n",
    "    \"ResNet50\":ResNet50,\n",
    "    \"Xception\":Xception,\n",
    "    \"InceptionResNetV2\":InceptionResNetV2,\n",
    "    \"InceptionV3\":InceptionV3,\n",
    "    # \"MobileNet\":MobileNet,\n",
    "    \"MobileNetV2\":MobileNetV2,\n",
    "    \"DenseNet121\":DenseNet121,\n",
    "    # \"DenseNet169\":DenseNet169,\n",
    "    \"DenseNet201\":DenseNet201,\n",
    "    \"NASNetLarge\":NASNetLarge,\n",
    "    # \"NASNetMobile\":NASNetMobile\n",
    "}\n",
    "\n",
    "input_size = {\n",
    "    \"VGG16\":(224,224),\n",
    "    \"VGG19\":(224,224),\n",
    "    \"ResNet50\":(224,224),\n",
    "    \"Xception\":(299,299),\n",
    "    \"InceptionResNetV2\":(299,299),\n",
    "    \"InceptionV3\":(299,299),\n",
    "    # \"MobileNet\":(224,224),\n",
    "    \"MobileNetV2\":(224,224),\n",
    "    \"DenseNet121\":(224,224),\n",
    "    # \"DenseNet169\":(224,224),\n",
    "    \"DenseNet201\":(224,224),\n",
    "    \"NASNetLarge\":(331,331),\n",
    "    # \"NASNetMobile\":(224,224)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dcm2np(dcm_path, target_size=(224, 224)):\n",
    "    \"\"\" Transforms a dcm into a np.array\n",
    "    \"\"\"\n",
    "    # --- Open DICOM file\n",
    "    d = pydicom.read_file(dcm_path)\n",
    "    im = d.pixel_array\n",
    "\n",
    "    # --- Convert from single-channel grayscale to 3-channel RGB\n",
    "    im = np.stack([im] * 3, axis=2)\n",
    "    \n",
    "    im = scipy.misc.imresize(im, target_size, interp='bilinear', mode=None)\n",
    "    \n",
    "    return im\n",
    "\n",
    "def _collect_models():\n",
    "    models = {}\n",
    "    for key,value in architectures.items() :\n",
    "        models[key] = Model(key, value, input_size[key], False, 'max')\n",
    "    return models\n",
    "\n",
    "\n",
    "def _generate_model(models):\n",
    "    for key,value in models.items():\n",
    "        #pos features\n",
    "        pos_df = _create_pretrained_feature_df(os.path.join(TRAIN_IMAGES, \"positive\"), \n",
    "                                               value, \n",
    "                                               partition=1.0,\n",
    "                                               target_size=input_size[key])\n",
    "\n",
    "        #neg features\n",
    "        neg_df = _create_pretrained_feature_df(os.path.join(TRAIN_IMAGES, \"negative\"), \n",
    "                                               value, \n",
    "                                               partition=0.25,\n",
    "                                               target_size=input_size[key])\n",
    "        \n",
    "        del value.Model\n",
    "\n",
    "def _get_feature_values(img_path, model, target_size):\n",
    "    \"\"\" Get the feature values for a specific image out of pre-trained model \n",
    "    \"\"\"\n",
    "#     img = dcm2np(img_path)\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return model.predict(x)[0].reshape(-1)\n",
    "\n",
    "def _create_pretrained_feature_df(class_directory, model, partition=1.0, target_size=(224, 224)):\n",
    "    \"\"\" Create features dataframe based on pretrained model \n",
    "    \"\"\"\n",
    "\n",
    "    features = []\n",
    "    patientId = []\n",
    "    \n",
    "    #instantiate model\n",
    "    model.CreateModel()\n",
    "    m = model.Model\n",
    "    \n",
    "    print('')\n",
    "    print('    ***')\n",
    "    print('Inference for {0} {1}'.format(class_directory, model.Name))\n",
    "    print('')\n",
    "    \n",
    "    path = os.path.normpath(class_directory)    \n",
    "    csv_saveto = path.split(os.sep)[-1]\n",
    "\n",
    "\n",
    "    def enumImgPath(class_directory,partition):\n",
    "        max = len(os.listdir(class_directory))\n",
    "        i=0\n",
    "        for n, file in enumerate(os.listdir(class_directory)):\n",
    "            if i > int(max*partition):\n",
    "                break\n",
    "            i+=1\n",
    "            yield file\n",
    "            \n",
    "    divisor = 50\n",
    "    n_max = len(os.listdir(class_directory))\n",
    "   \n",
    "    for n, file in enumerate(enumImgPath(class_directory,partition)):\n",
    "        full_path = os.path.join(class_directory, file)\n",
    "        features.append(_get_feature_values(full_path, m,target_size=target_size))\n",
    "        patientId.append(file)\n",
    "        \n",
    "        csv_path = os.path.join(TRAIN_FEATURES_DIR,csv_saveto,'{0}_{1:05d}.csv'.format(model.Name,n))\n",
    "        \n",
    "        if( (n+1) % (n_max/divisor) ==0 ):\n",
    "            print('Scoring image {0}'.format(n/n_max))\n",
    "            df = pd.DataFrame(np.stack(features))\n",
    "            df['patientID'] = patientId\n",
    "            df.to_csv(path_or_buf=csv_path)\n",
    "            print('Saved file {0}'.format(csv_path))\n",
    "            features = []\n",
    "\n",
    "    df = pd.DataFrame(np.stack(features))\n",
    "    df['patientID'] = patientId\n",
    "    df.to_csv(path_or_buf=csv_path)\n",
    "    print('Saved file {0}'.format(csv_path))\n",
    "            \n",
    "    return pd.DataFrame(np.stack(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = _collect_models()\n",
    "_generate_model(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"Nearest Neighbors\":KNeighborsClassifier(3),\n",
    "    \"Linear SVM\":SVC(kernel=\"linear\", C=0.025, probability=True),\n",
    "    \"RBF SVM\":SVC(gamma=2, C=1, probability=True),\n",
    "    \"Gaussian Process\":GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),\n",
    "    \"Decision Tree\":DecisionTreeClassifier(max_depth=5),\n",
    "    \"Random Forest\":RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    \"Neural Net\":MLPClassifier(alpha=1),\n",
    "    \"AdaBoost\":AdaBoostClassifier(),\n",
    "    \"Naive Bayes\":GaussianNB(),\n",
    "    \"QDA\":QuadraticDiscriminantAnalysis()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clf', 'negative', 'positive']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(TRAIN_FEATURES_DIR)[:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "subset = 100000\n",
    "\n",
    "metadata = pd.read_csv(os.path.join(TRAIN_IMAGES, 'metadata.csv'))\n",
    "metadata = metadata[['patient_id', 'patients_age']]\n",
    "df_neg = pd.DataFrame({'patientId': [os.path.splitext(f)[0] for f in os.listdir(TRAIN_IMAGES_NEGATIVE)[:subset]]})\n",
    "df_pos = pd.DataFrame({'patientId': [os.path.splitext(f)[0] for f in os.listdir(TRAIN_IMAGES_POSITIVE)[:subset]]})\n",
    "\n",
    "names = []\n",
    "for _df in (df_neg, df_pos):\n",
    "    for i, row in _df.iterrows():\n",
    "        r = metadata.loc[metadata['patient_id'] == row['patientId']]\n",
    "        if len(r) == 0:\n",
    "            names.append(row['patientId'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_df = df_neg = pd.DataFrame({'patientId': [os.path.splitext(f)[0] for f in os.listdir(TRAIN_IMAGES_NEGATIVE)]})\n",
    "_df = pd.concat([_df, pd.DataFrame({'patientId': [os.path.splitext(f)[0] for f in os.listdir(TRAIN_IMAGES_POSITIVE)]})])\n",
    "\n",
    "metadatas = []\n",
    "\n",
    "for i, row in _df.iterrows():\n",
    "    # --- Open DICOM file\n",
    "    dcm_path = os.path.join(TRAIN_DCM, row['patientId']) + '.dcm'\n",
    "    dcm_file = pydicom.read_file(dcm_path)\n",
    "    raw_dict = {x.description(): x.value for x in dcm_file.iterall() if x.description() != \"Pixel Data\"}\n",
    "    for key in raw_dict.keys():\n",
    "        new_key = key.replace(\"'\", \"\").replace(\" \", \"_\").lower()\n",
    "        raw_dict[new_key] = raw_dict.pop(key)\n",
    "    \n",
    "    metadatas.append(raw_dict)\n",
    "    \n",
    "df = pd.DataFrame(metadatas)\n",
    "df.to_csv(os.path.join(DATA_DIR, 'metadata.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReadFeatureModel(name):\n",
    "\n",
    "    subset = 5000\n",
    "    print(f'subset: {subset}')\n",
    "    \n",
    "    # read negative/positive\n",
    "    df_positive = pd.read_csv(os.path.join(TRAIN_FEATURES_DIR, 'positive', f'{name}_05658.csv'), index_col=0)\n",
    "    df_negative = pd.read_csv(os.path.join(TRAIN_FEATURES_DIR, 'negative', f'{name}_05006.csv'), index_col=0)\n",
    "    df_negative = df_negative.iloc[:len(df_positive)]\n",
    "\n",
    "    df_positive = df_positive.iloc[:subset]\n",
    "    df_negative = df_negative.iloc[:subset]\n",
    "\n",
    "    df_positive['Target'] = 1\n",
    "    df_negative['Target'] = 0\n",
    "\n",
    "    df_positive['patientId'] = [os.path.splitext(f)[0] for f in os.listdir(TRAIN_IMAGES_POSITIVE)[:subset]]\n",
    "    df_negative['patientId'] = [os.path.splitext(f)[0] for f in os.listdir(TRAIN_IMAGES_NEGATIVE)[:subset]]\n",
    "\n",
    "    # append metadata to negative/positive\n",
    "    if(False):\n",
    "        metadata = pd.read_csv(os.path.join(TRAIN_IMAGES, 'metadata.csv'))\n",
    "        metadata = metadata[['patient_id', 'patients_age']]\n",
    "        df_neg = pd.DataFrame({'patientId': [os.path.splitext(f)[0] for f in os.listdir(TRAIN_IMAGES_NEGATIVE)[:subset]]})\n",
    "        df_pos = pd.DataFrame({'patientId': [os.path.splitext(f)[0] for f in os.listdir(TRAIN_IMAGES_POSITIVE)[:subset]]})\n",
    "\n",
    "        for _df in (df_neg, df_pos):\n",
    "            ages = []\n",
    "            for i, row in _df.iterrows():\n",
    "                r = metadata.loc[metadata['patient_id'] == row['patientId']]\n",
    "                ages.append(r['patients_age'].values[0])\n",
    "\n",
    "            _df['age'] = ages\n",
    "\n",
    "        df_positive['age'] = df_pos['age']\n",
    "        df_negative['age'] = df_neg['age']\n",
    "    elif(False):\n",
    "        for _df in (df_positive, df_negative):\n",
    "            ages = []\n",
    "            for i, row in _df.iterrows():\n",
    "                # --- Open DICOM file\n",
    "                dcm_path = os.path.join(TRAIN_DCM, row['patientId']) + '.dcm'\n",
    "                d = pydicom.read_file(dcm_path)\n",
    "\n",
    "                ages.append(d.PatientAge)\n",
    "\n",
    "            _df['age'] = ages\n",
    "    else:\n",
    "        metadata = pd.read_csv(os.path.join(DATA_DIR, 'metadata.csv'))\n",
    "        metadata = metadata[['Patient ID', 'Patient\\'s Age']]\n",
    "        df_neg = pd.DataFrame({'patientId': [os.path.splitext(f)[0] for f in os.listdir(TRAIN_IMAGES_NEGATIVE)[:subset]]})\n",
    "        df_pos = pd.DataFrame({'patientId': [os.path.splitext(f)[0] for f in os.listdir(TRAIN_IMAGES_POSITIVE)[:subset]]})\n",
    "\n",
    "        for _df in (df_neg, df_pos):\n",
    "            ages = []\n",
    "            for i, row in _df.iterrows():\n",
    "                r = metadata.loc[metadata['Patient ID'] == row['patientId']]\n",
    "                ages.append(r['Patient\\'s Age'].values[0])\n",
    "\n",
    "            _df['age'] = ages\n",
    "\n",
    "        df_positive['age'] = df_pos['age']\n",
    "        df_negative['age'] = df_neg['age']\n",
    "\n",
    "    # concatenate the two datasets\n",
    "    df = pd.concat([df_positive, df_negative])\n",
    "\n",
    "    return df\n",
    "\n",
    "#     df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare feature dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PrepareDataSet(df):\n",
    "    X = df.drop(['Target', 'patientId'], axis=1)\n",
    "    Y = df['Target']\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.4, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          index=0):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    rows=3\n",
    "    ax = plt.subplot((len(classifiers) // rows) +1, rows, index+1)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Classify(X_train, X_test, y_train, y_test):\n",
    "    fig = plt.figure(figsize=(13, 13))\n",
    "\n",
    "    #X_train, X_test, y_train, y_test\n",
    "    for i, (name, clf) in enumerate(classifiers.items()):\n",
    "        print('{0} - {1}'.format(datetime.datetime.now(), name))\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        train_preds = clf.predict_proba(X_train)\n",
    "        test_preds = clf.predict_proba(X_test)\n",
    "        roc_train = roc_auc_score(y_train, train_preds[:,1])\n",
    "        roc_test = roc_auc_score(y_test, test_preds[:,1])\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        np.set_printoptions(precision=2)\n",
    "\n",
    "        # Plot normalized confusion matrix\n",
    "\n",
    "        plot_confusion_matrix(cnf_matrix, \n",
    "                              classes=['normal', 'recession'], \n",
    "                              normalize=True,\n",
    "                              title='{0} - {1:.3f}\\nROC train {2:.3f}, ROC test {3:.3f}'.format(name, score, roc_train, roc_test),\n",
    "                              index=i)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DenseNet121\n",
      "subset: 5000\n",
      "2018-09-18 21:37:18.153881 - Nearest Neighbors\n"
     ]
    }
   ],
   "source": [
    "for name in os.listdir(os.path.join(TRAIN_FEATURES_DIR, 'positive')):\n",
    "    name = os.path.splitext(name)[0]\n",
    "    name = name.split(sep='_')[0]\n",
    "    \n",
    "    print('--- {0}'.format(name))\n",
    "    \n",
    "    df = ReadFeatureModel(name)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = PrepareDataSet(df)\n",
    "    \n",
    "    fig = Classify(X_train, X_test, y_train, y_test)\n",
    "        \n",
    "    # save results\n",
    "    clf_path = os.path.join(CLF_DIR, name)\n",
    "    if not os.path.exists(clf_path):\n",
    "        os.makedirs(clf_path)\n",
    "    \n",
    "    fig.savefig(os.path.join(clf_path, '{0}_benchmark.png'.format(name)))\n",
    "    \n",
    "    for clf_name, clf in classifiers.items():        \n",
    "        joblib.dump(clf, '{0}/{1}_{2}.pkl'.format(clf_path, name, clf_name))\n",
    "    \n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_predictions = pd.DataFrame()\n",
    "\n",
    "for i, (name, clf) in enumerate(classifiers.items()):\n",
    "    print('{0} - {1}'.format(datetime.datetime.now(), name))\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    train_preds = clf.predict_proba(X_train)\n",
    "    test_preds = clf.predict_proba(X_test)\n",
    "    roc_train = roc_auc_score(y_train, train_preds[:,1])\n",
    "    roc_test = roc_auc_score(y_test, test_preds[:,1])\n",
    "             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier, Perceptron, PassiveAggressiveRegressor, LogisticRegression\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV, LinearRegression\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "heldout = [0.95, 0.90, 0.75, 0.50, 0.01]\n",
    "rounds = 20\n",
    "\n",
    "\n",
    "    \n",
    "class XgbWrapper(object):\n",
    "    def __init__(self, seed=0):\n",
    "        self.params = {\"objective\": \"reg:linear\", \"booster\":\"gblinear\"}\n",
    "        self.nrounds = 250\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        self.gbdt = xgb.train(self.params, dtrain, self.nrounds)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.gbdt.predict(xgb.DMatrix(x))\n",
    "\n",
    "classifiers = {\n",
    "    \"PLSRegression\": PLSRegression(n_components=2),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"RidgeCV\": RidgeCV(),\n",
    "    \"MLPRegressor\": MLPRegressor(max_iter=200,hidden_layer_sizes=(50)),\n",
    "    \"KNeighborsRegressor\": KNeighborsRegressor(n_neighbors=5 ),\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"ElasticNet\": ElasticNet(),\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rsna]",
   "language": "python",
   "name": "conda-env-rsna-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
