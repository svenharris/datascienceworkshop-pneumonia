{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is 2AEF-42D6\n",
      "\n",
      " Directory of C:\\dev\\datascienceworkshop-pneumonia\n",
      "\n",
      "13/10/2018  15:30    <DIR>          .\n",
      "13/10/2018  15:30    <DIR>          ..\n",
      "18/09/2018  21:40             1,328 .gitignore\n",
      "13/10/2018  13:52    <DIR>          .ipynb_checkpoints\n",
      "14/09/2018  00:18    <DIR>          .vscode\n",
      "13/10/2018  13:34            25,589 CheckEnvironment.ipynb\n",
      "27/09/2018  22:05               491 config.py\n",
      "13/10/2018  13:34         3,603,514 create_patient_JSONs.ipynb\n",
      "13/10/2018  13:34    <DIR>          data\n",
      "27/09/2018  22:05    <DIR>          data_preprocessing\n",
      "09/09/2018  20:42           387,595 kernel.ipynb\n",
      "13/10/2018  13:34           228,675 kernel_LG.ipynb\n",
      "13/10/2018  15:30    <DIR>          logs\n",
      "14/09/2018  00:18    <DIR>          models\n",
      "13/10/2018  15:30            24,600 obj_detection_rcnn.ipynb\n",
      "13/10/2018  15:29    <DIR>          output\n",
      "13/10/2018  13:34            65,775 Pneumonia_Training.ipynb\n",
      "09/09/2018  20:38                31 README.md\n",
      "09/09/2018  20:42             1,954 req.txt\n",
      "13/10/2018  13:34    <DIR>          resources\n",
      "12/09/2018  21:06    <DIR>          __pycache__\n",
      "              10 File(s)      4,339,552 bytes\n",
      "              11 Dir(s)  197,578,727,424 bytes free\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import keras\n",
    "from keras_rcnn import datasets, models, preprocessing, utils\n",
    "from keras_rcnn.datasets import shape\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = r'data'\n",
    "JSON_FILE = os.path.join(DATA_PATH,r'train_positive.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': {'checksum': '14a05c5cd4498a9beb3bc9ca555c8cd0',\n",
       "  'pathname': 'data/train_images/positive/4525bf0b-20b4-4f4f-9149-e32632812d89.png',\n",
       "  'shape': {'r': 1024, 'c': 1024, 'channels': 1}},\n",
       " 'objects': [{'bounding_box': {'minimum': {'r': 301, 'c': 403},\n",
       "    'maximum': {'r': 491, 'c': 757}},\n",
       "   'category': 'sick'},\n",
       "  {'bounding_box': {'minimum': {'r': 533, 'c': 359},\n",
       "    'maximum': {'r': 788, 'c': 755}},\n",
       "   'category': 'sick'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "\n",
    "with open(JSON_FILE) as j:\n",
    "    data = json.load(j)\n",
    "    \n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_shuffled = data[:]\n",
    "\n",
    "portion = int(0.8*len(data_shuffled))\n",
    "\n",
    "#shuffle  list\n",
    "random.seed(55)\n",
    "random.shuffle(data_shuffled)\n",
    "\n",
    "training_dictionary = data_shuffled[:20]\n",
    "test_dictionary = data_shuffled[20:30]\n",
    "\n",
    "epoch_size = len(training_dictionary)\n",
    "epoch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = {\"sick\": 1}\n",
    "\n",
    "generator = preprocessing.ObjectDetectionGenerator()\n",
    "\n",
    "generator = generator.flow_from_dictionary(\n",
    "    dictionary=training_dictionary,\n",
    "    categories=categories,\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb'\n",
    ")\n",
    "\n",
    "validation_data = preprocessing.ObjectDetectionGenerator()\n",
    "\n",
    "validation_data = validation_data.flow_from_dictionary(\n",
    "    dictionary=test_dictionary,\n",
    "    categories=categories,\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb'\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "target,_ = generator.next()\n",
    "\n",
    "target_bounding_boxes, target_categories, target_images, target_masks, target_metadata = target\n",
    "\n",
    "target_bounding_boxes = numpy.squeeze(target_bounding_boxes)\n",
    "\n",
    "target_images = numpy.squeeze(target_images)\n",
    "\n",
    "target_categories = numpy.argmax(target_categories, -1)\n",
    "\n",
    "target_categories = numpy.squeeze(target_categories)\n",
    "\n",
    "if(len(target_bounding_boxes.shape) != 2):\n",
    "    target_bounding_boxes = numpy.array([target_bounding_boxes])\n",
    "print(target_bounding_boxes.shape)\n",
    "utils.show_bounding_boxes(target_images, target_bounding_boxes, target_categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create RCNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://github.com/broadinstitute/keras-rcnn](https://github.com/broadinstitute/keras-rcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keras_rcnn.\n",
    "model = models.RCNN((224, 224, 3), [\"pneumonia\"])\n",
    "\n",
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras callbacks  \n",
    "[https://keras.io/callbacks/](https://keras.io/callbacks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, TerminateOnNaN, TensorBoard, LearningRateScheduler, ModelCheckpoint\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SGDRScheduler(TensorBoard):\n",
    "    '''Cosine annealing learning rate scheduler with periodic restarts.\n",
    "    # Usage\n",
    "        ```python\n",
    "            schedule = SGDRScheduler(min_lr=1e-5,\n",
    "                                     max_lr=1e-2,\n",
    "                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n",
    "                                     lr_decay=0.9,\n",
    "                                     cycle_length=5,\n",
    "                                     mult_factor=1.5)\n",
    "            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n",
    "        ```\n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        lr_decay: Reduce the max_lr after the completion of each cycle.\n",
    "                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n",
    "        cycle_length: Initial number of epochs in a cycle.\n",
    "        mult_factor: Scale epochs_to_restart after each full cycle completion.\n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: http://arxiv.org/abs/1608.03983\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 min_lr,\n",
    "                 max_lr,\n",
    "                 steps_per_epoch,\n",
    "                 lr_decay=1,\n",
    "                 cycle_length=10,\n",
    "                 mult_factor=2):\n",
    "\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "\n",
    "        self.history = {}\n",
    "        \n",
    "        # tensorboard\n",
    "        log_dir = './logs'\n",
    "        super(SGDRScheduler, self).__init__(log_dir, histogram_freq=0, batch_size=16, write_graph=True, write_grads=True, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)\n",
    "        self.lr_log_dir = log_dir\n",
    "\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
    "        return lr\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        \n",
    "        # tensorboard\n",
    "        lr = self.clr()\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag='lr', simple_value=lr),\n",
    "                                    tf.Summary.Value(tag='loss', simple_value=logs.get('loss')),\n",
    "                                    tf.Summary.Value(tag='val_loss', simple_value=logs.get('val_loss'))])\n",
    "        self.lr_writer.add_summary(summary, epoch)\n",
    "        self.lr_writer.flush()\n",
    "        super(SGDRScheduler, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        self.model.set_weights(self.best_weights)\n",
    "        \n",
    "        # tensorboard\n",
    "        super(SGDRScheduler, self).on_train_end(logs)\n",
    "        self.lr_writer.close()\n",
    "    \n",
    "    def set_model(self, model):\n",
    "        self.lr_writer = tf.summary.FileWriter(self.lr_log_dir)\n",
    "        super(SGDRScheduler, self).set_model(model)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss history\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "history = LossHistory()\n",
    "\n",
    "# terminate on NaN\n",
    "term_nan = TerminateOnNaN()    \n",
    "\n",
    "# checkpoint\n",
    "checkpoint_filename = os.path.join('C:\\dev\\datascienceworkshop-pneumonia\\output', 'weights.{epoch:02d}-{val_loss:.2f}.hdf5')\n",
    "checkpoint = ModelCheckpoint(checkpoint_filename, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "# Tensorboard\n",
    "tb = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=16, write_graph=True, write_grads=True, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)\n",
    "\n",
    "# Learning rate reset\n",
    "def UpdateLR(i, lr):\n",
    "    if (i == 0):\n",
    "        return 0.001\n",
    "    elif(i < 10):\n",
    "        return lr*0.9\n",
    "    elif(i < 20):\n",
    "        return lr*0.8\n",
    "lrs = LearningRateScheduler(schedule=UpdateLR, verbose=1)\n",
    "\n",
    "# Cosine annealing reset\n",
    "batch_size=16\n",
    "sgdr = SGDRScheduler(min_lr=1e-5,\n",
    "                         max_lr=1e-3,\n",
    "                         steps_per_epoch=np.ceil(epoch_size/batch_size),\n",
    "                         lr_decay=0.9,\n",
    "                         cycle_length=5,\n",
    "                         mult_factor=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a terminal, run:  \n",
    "\n",
    "```tensorboard --logdir=C:/dev/datascienceworkshop-pneumonia/logs```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "webbrowser.open('http://desktop-in5570r:6006')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loren\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loren\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\skimage\\transform\\_warps.py:24: UserWarning: The default multichannel argument (None) is deprecated.  Please specify either True or False explicitly.  multichannel will default to False starting with release 0.16.\n",
      "  warn('The default multichannel argument (None) is deprecated.  Please '\n",
      "C:\\Users\\loren\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\skimage\\transform\\_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "C:\\Users\\loren\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\skimage\\transform\\_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 27s 1s/step - loss: 65.4348 - val_loss: 2241778.5192\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 20s 976ms/step - loss: 2.6959 - val_loss: 1915414.1318\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 19s 973ms/step - loss: 1.8244 - val_loss: 1363598.2264\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 19s 970ms/step - loss: 1.5735 - val_loss: 600140.3603\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 20s 981ms/step - loss: 1.3621 - val_loss: 33680.4724\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 19s 974ms/step - loss: 1.3056 - val_loss: 5704.9190\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 19s 970ms/step - loss: 1.3472 - val_loss: 351256.8469\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 20s 1s/step - loss: 1.2238 - val_loss: 2569.0207\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 20s 1s/step - loss: 1.2862 - val_loss: 9735.4608\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 20s 1s/step - loss: 1.2144 - val_loss: 11933.6365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18f048c4828>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    epochs=10,\n",
    "    generator=generator,\n",
    "    validation_data=validation_data,\n",
    "    callbacks=[sgdr, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import skimage\n",
    "\n",
    "image = skimage.io.imread(r'data/test_images/00a05408-8291-4231-886e-13763e103161.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "image.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
